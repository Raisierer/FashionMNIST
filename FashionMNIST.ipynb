{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdeeplearning2020condab398666adea745a0bd5e7ec854d90aad",
   "display_name": "Python 3.7.6 64-bit ('deeplearning2020': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense                                                                   \n",
    "from keras.layers import Conv2D, AveragePooling2D, Dropout, Flatten      \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128                                                                                 \n",
    "num_classes = 10                                                                                 \n",
    "epochs = 12                                                                                      \n",
    "img_rows, img_cols = 28, 28                                                                      \n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_images = np.pad(train_images, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "test_images = np.pad(test_images, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "train_vec_labels = keras.utils.to_categorical(train_labels, total_classes)\n",
    "test_vec_labels = keras.utils.to_categorical(test_labels, total_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
    }
   ],
   "source": [
    "print(train_vec_labels[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architectures"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet5():    \n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters=6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(32,32,1)))\n",
    "    model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Conv2D(filters=6, kernel_size=(5, 5), padding='VALID', activation='relu'))\n",
    "    model.add(AveragePooling2D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "    model.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "    model.add(Dense(units=10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train Network"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_20\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_42 (Conv2D)           (None, 28, 28, 64)        1664      \n_________________________________________________________________\naverage_pooling2d_35 (Averag (None, 14, 14, 64)        0         \n_________________________________________________________________\nconv2d_43 (Conv2D)           (None, 10, 10, 32)        51232     \n_________________________________________________________________\naverage_pooling2d_36 (Averag (None, 5, 5, 32)          0         \n_________________________________________________________________\nflatten_18 (Flatten)         (None, 800)               0         \n_________________________________________________________________\ndense_52 (Dense)             (None, 120)               96120     \n_________________________________________________________________\ndense_53 (Dense)             (None, 84)                10164     \n_________________________________________________________________\ndense_54 (Dense)             (None, 10)                850       \n=================================================================\nTotal params: 160,030\nTrainable params: 160,030\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 37s 612us/step - loss: 0.0900 - accuracy: 0.1428 - val_loss: 0.0900 - val_accuracy: 0.1601\nEpoch 2/12\n60000/60000 [==============================] - 37s 618us/step - loss: 0.0899 - accuracy: 0.1583 - val_loss: 0.0899 - val_accuracy: 0.1595\nEpoch 3/12\n60000/60000 [==============================] - 38s 639us/step - loss: 0.0898 - accuracy: 0.1544 - val_loss: 0.0898 - val_accuracy: 0.1538\nEpoch 4/12\n60000/60000 [==============================] - 37s 620us/step - loss: 0.0897 - accuracy: 0.1555 - val_loss: 0.0897 - val_accuracy: 0.1645\nEpoch 5/12\n60000/60000 [==============================] - 37s 609us/step - loss: 0.0896 - accuracy: 0.1734 - val_loss: 0.0895 - val_accuracy: 0.1788\nEpoch 6/12\n60000/60000 [==============================] - 37s 616us/step - loss: 0.0895 - accuracy: 0.1872 - val_loss: 0.0894 - val_accuracy: 0.1965\nEpoch 7/12\n60000/60000 [==============================] - 37s 609us/step - loss: 0.0893 - accuracy: 0.2066 - val_loss: 0.0892 - val_accuracy: 0.2189\nEpoch 8/12\n60000/60000 [==============================] - 37s 621us/step - loss: 0.0891 - accuracy: 0.2184 - val_loss: 0.0889 - val_accuracy: 0.2198\nEpoch 9/12\n60000/60000 [==============================] - 37s 622us/step - loss: 0.0888 - accuracy: 0.2144 - val_loss: 0.0886 - val_accuracy: 0.2089\nEpoch 10/12\n60000/60000 [==============================] - 40s 660us/step - loss: 0.0884 - accuracy: 0.2036 - val_loss: 0.0882 - val_accuracy: 0.1998\nEpoch 11/12\n60000/60000 [==============================] - 39s 645us/step - loss: 0.0879 - accuracy: 0.1926 - val_loss: 0.0876 - val_accuracy: 0.1848\nEpoch 12/12\n60000/60000 [==============================] - 39s 643us/step - loss: 0.0872 - accuracy: 0.1814 - val_loss: 0.0868 - val_accuracy: 0.1844\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1e11e27b148>"
     },
     "metadata": {},
     "execution_count": 135
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_vec_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=True,\n",
    "          validation_data=(test_images, test_vec_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model accuracy: 0.29\n"
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = model.evaluate(test_images, test_vec_labels, verbose=False)\n",
    "print(\"Model accuracy: %.2f\" % eval_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}